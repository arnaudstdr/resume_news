#!/usr/bin/env python
# filepath: /workspaces/veille_ai/scripts/summarizer/weekly_digest.py
import os
import sys
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import dotenv
import requests

# Ajouter le r√©pertoire parent au chemin pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.db_manager import DatabaseManager

# Charger automatiquement les variables d'environnement depuis .env
dotenv.load_dotenv()

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('weekly_digest.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class WeeklyDigest:
    """
    Classe pour g√©n√©rer un r√©sum√© hebdomadaire des articles scrap√©s
    en utilisant un LLM open source compatible text-generation.
    """

    def __init__(self, db_path: Optional[str] = None):
        """
        Initialise le g√©n√©rateur de r√©sum√© hebdomadaire.
        
        Args:
            db_path: Chemin vers la base de donn√©es SQLite
        """
        if db_path is None:
            db_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 
                                  "data", "rss_articles.db")
            
        self.db_manager = DatabaseManager(db_path)
        self.db_manager.connect()  # Connexion √† la base de donn√©es d√®s l'initialisation
        logger.info(f"Initialisation du digest hebdomadaire")
        
    def get_weekly_articles(self, days: int = 7, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        R√©cup√®re les articles publi√©s au cours des derniers jours.
        
        Args:
            days: Nombre de jours √† consid√©rer
            limit: Nombre maximum d'articles √† r√©cup√©rer (None = pas de limite raisonnable)
            
        Returns:
            Liste des articles r√©cents
        """
        try:
            # Suppression de l'ouverture/fermeture de connexion ici
            articles = self.db_manager.get_recent_articles(limit=1000 if limit is None else limit, days=days)
            logger.info(f"R√©cup√©ration de {len(articles)} articles des {days} derniers jours")
            return articles
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des articles: {str(e)}")
            return []
    
    def prepare_content_for_digest(self, articles: List[Dict[str, Any]]) -> str:
        """
        Pr√©pare le contenu des articles pour la g√©n√©ration du r√©sum√©.
        Utilise en priorit√© le r√©sum√©, puis la description, puis le contenu.
        
        Args:
            articles: Liste des articles √† r√©sumer
            
        Returns:
            Texte format√© pour √™tre envoy√© au mod√®le
        """
        content_list = []
        
        # Regrouper les articles par source
        articles_by_source = {}
        for article in articles:
            source = article.get('source', 'Unknown')
            if source not in articles_by_source:
                articles_by_source[source] = []
            articles_by_source[source].append(article)
        
        # Formater les articles par source
        for source, source_articles in articles_by_source.items():
            content_list.append(f"\n## Articles de {source} ({len(source_articles)} articles)")
            
            for article in source_articles:
                title = article.get('title', 'Sans titre')
                summary = article.get('summary', '')
                description = article.get('description', '')
                content = article.get('content', '')
                url = article.get('url', '')
                date = article.get('date', '')
                
                # Formatage de la date si disponible
                if date:
                    try:
                        date_obj = datetime.fromisoformat(date.replace('Z', '+00:00'))
                        date_str = date_obj.strftime("%d/%m/%Y")
                        content_list.append(f"\n### [{title}]({url}) - {date_str}")
                    except:
                        content_list.append(f"\n### [{title}]({url})")
                else:
                    content_list.append(f"\n### [{title}]({url})")
                
                # Utiliser le meilleur contenu disponible
                if summary and len(summary) > 50:
                    content_list.append(f"R√©sum√© : {summary}")
                elif description and len(description) > 50:
                    content_list.append(f"Description : {description}")
                elif content and len(content) > 50:
                    # Limiter la taille du contenu pour ne pas d√©passer le contexte du mod√®le
                    content = content[:2000] + "..." if len(content) > 2000 else content
                    content_list.append(f"Contenu : {content}")
                
        return "\n".join(content_list)
    
    
    def ollama_generate(self, prompt: str, model: str = "mistral", max_tokens: int = 3500) -> str:
        """
        Utilise l'API locale d'Ollama pour g√©n√©rer du texte avec le mod√®le sp√©cifi√©.
        """
        try:
            response = requests.post(
                "http://host.docker.internal:11434/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"num_predict": max_tokens}
                },
                timeout=180
            )
            response.raise_for_status()
            data = response.json()
            return data.get("response", "")
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration avec Ollama : {str(e)}")
            return f"Erreur lors de la g√©n√©ration avec Ollama : {str(e)}"
    
    def mistral_generate(self, prompt: str, max_tokens: int = 3500) -> str:
        """
        Utilise l'API HTTP de Mistral Large pour g√©n√©rer du texte avec la cl√© API stock√©e dans .env.
        """
        api_key = os.environ.get("MISTRAL_API_KEY")
        if not api_key:
            logger.error("Cl√© API Mistral manquante dans .env (MISTRAL_API_KEY)")
            return "Erreur : cl√© API Mistral manquante."
        try:
            url = "https://api.mistral.ai/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            data = {
                "model": "mistral-large-latest",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": 0.7
            }
            response = requests.post(url, headers=headers, json=data, timeout=180)
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"]
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration avec Mistral Large : {str(e)}")
            return f"Erreur lors de la g√©n√©ration avec Mistral Large : {str(e)}"

    def generate_digest(self, days: int = 7, max_tokens: int = 3500, limit: Optional[int] = None, use_mistral: bool = True) -> str:
        """
        G√©n√®re un r√©sum√© hebdomadaire des articles r√©cents.
        
        Args:
            days: Nombre de jours √† consid√©rer
            max_tokens: Nombre maximum de tokens pour la g√©n√©ration (d√©faut: 3000)
            limit: Nombre maximum d'articles √† r√©sumer (None = pas de limite raisonnable)
            use_mistral: Utiliser l'API Mistral (True) ou Ollama (False)
            
        Returns:
            R√©sum√© hebdomadaire au format texte
        """
        try:
            # R√©cup√©rer les articles
            # Note: La connexion est d√©j√† √©tablie dans l'initialiseur
            articles = self.db_manager.get_recent_articles(limit=1000 if limit is None else limit, days=days)
            
            if not articles:
                logger.warning("Aucun article trouv√© pour la p√©riode sp√©cifi√©e.")
                self.db_manager.disconnect()  # Assurons-nous de fermer la connexion
                return "Aucun article disponible pour cette semaine."
            
            # V√©rifier si au moins quelques articles ont du contenu ou un r√©sum√©
            articles_with_content = [a for a in articles if a.get('content') or a.get('summary')]
            if not articles_with_content and len(articles) > 0:
                logger.warning("Aucun article n'a de contenu ou de r√©sum√©. Utilisation des titres uniquement.")
                # Adapter pour utiliser uniquement les titres
                for a in articles:
                    a['content'] = f"{a.get('title', 'Sans titre')}"
            
            # Pr√©parer le contenu
            content = self.prepare_content_for_digest(articles)
            
            # Date de d√©but et de fin pour le titre
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            date_range = f"{start_date.strftime('%d/%m/%Y')} au {end_date.strftime('%d/%m/%Y')}"
            
            # Cr√©er le prompt pour Mistral (en fran√ßais)
            prompt = f"""Tu es un expert en intelligence artificielle charg√© de r√©diger un r√©sum√© hebdomadaire clair et exhaustif sur les actualit√©s r√©centes en IA.

Voici les articles publi√©s du {date_range} sur l‚Äôintelligence artificielle :

{content}

R√©dige un r√©sum√© approfondi et strat√©gique en fran√ßais, en suivant cette structure simple :

üóûÔ∏è Synth√®se hebdomadaire
	‚Ä¢	Un r√©sum√© d√©taill√© de 400-500 mots des principales actualit√©s et tendances importantes de la semaine.
	‚Ä¢	Mentionne clairement ce qui est particuli√®rement impactant, innovant ou strat√©gique dans le domaine de l‚ÄôIA.

üìå Lectures compl√©mentaires
	‚Ä¢	Une liste simple avec les liens directs vers les articles les plus pertinents pour approfondir.

Directives :
	‚Ä¢	Priorise la clart√©, la pertinence et l‚Äôexhaustivit√©.
	‚Ä¢	Style : professionnel, synth√©tique, facile √† lire rapidement.
	‚Ä¢	Format : Markdown structur√© avec titres et listes √† puces.
	‚Ä¢	Langue : Fran√ßais clair et accessible.
"""
            
            logger.info(f"G√©n√©ration du r√©sum√© hebdomadaire avec Mistral Large...")
            if use_mistral:
                response = self.mistral_generate(prompt, max_tokens=max_tokens)
            else:
                response = self.ollama_generate(prompt, model="mistral", max_tokens=max_tokens)
            
            # D√©connexion de la base de donn√©es
            self.db_manager.disconnect()
            
            if not response or len(response) < 50:
                logger.error(f"R√©ponse trop courte ou vide: '{response}'")
                return f"# Erreur de g√©n√©ration du r√©sum√©\n\nAucune r√©ponse de qualit√© g√©n√©r√©e. R√©ponse re√ßue: {response}"
                
            header = f"# Veille IA: R√©sum√© hebdomadaire du {date_range}\n\n"
            return header + response
                
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration du r√©sum√©: {str(e)}", exc_info=True)
            # S'assurer que la connexion est ferm√©e m√™me en cas d'erreur
            try:
                self.db_manager.disconnect()
            except:
                pass
            return f"Erreur lors de la g√©n√©ration du r√©sum√©: {str(e)}"
    
    def save_digest(self, digest: str, output_dir: Optional[str] = None) -> str:
        """
        Sauvegarde le r√©sum√© dans un fichier markdown.
        
        Args:
            digest: Contenu du r√©sum√©
            output_dir: R√©pertoire de sortie (par d√©faut: outputs/normalized/digest_hebdo)
            
        Returns:
            Chemin du fichier de sortie
        """
        if output_dir is None:
            output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "outputs", "normalized", "digest_hebdo")
        os.makedirs(output_dir, exist_ok=True)
        filename = f"digest_hebdo_{datetime.now().strftime('%Y%m%d')}.md"
        output_path = os.path.join(output_dir, filename)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(digest)
        logger.info(f"R√©sum√© hebdomadaire sauvegard√© dans {output_path}")
        return output_path

    def save_full_articles_markdown(self, days: int = 7, output_dir: Optional[str] = None, limit: Optional[int] = None) -> str:
        """
        G√©n√®re un fichier markdown listant tous les articles scrapp√©s de la semaine avec toutes les infos (titre, contenu, lien, etc.).
        Args:
            days: Nombre de jours √† consid√©rer
            output_dir: R√©pertoire de sortie (d√©faut: outputs/normalized/articles_complets)
            limit: Nombre max d'articles (None = pas de limite raisonnable)
        Returns:
            Chemin du fichier markdown g√©n√©r√©
        """
        if output_dir is None:
            output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "outputs", "normalized", "articles_complets")
        os.makedirs(output_dir, exist_ok=True)
        articles = self.db_manager.get_recent_articles(limit=1000 if limit is None else limit, days=days)
        if not articles:
            logger.warning("Aucun article trouv√© pour la g√©n√©ration du markdown complet.")
            return ""
        filename = f"articles_complets_{datetime.now().strftime('%Y%m%d')}.md"
        output_path = os.path.join(output_dir, filename)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(f"# Articles scrapp√©s de la semaine ({len(articles)})\n\n")
            for idx, article in enumerate(articles, 1):
                title = article.get('title', 'Sans titre')
                url = article.get('url', '')
                date = article.get('date', '')
                content = article.get('content', '')
                summary = article.get('summary', '')
                description = article.get('description', '')
                source = article.get('source_name', article.get('source', 'Inconnu'))
                f.write(f"## {idx}. [{title}]({url})\n")
                if date:
                    f.write(f"**Date :** {date}\n\n")
                f.write(f"**Source :** {source}\n\n")
                if summary:
                    f.write(f"**R√©sum√© :** {summary}\n\n")
                if description:
                    f.write(f"**Description :** {description}\n\n")
                f.write(f"**Contenu :**\n\n{content}\n\n")
                f.write("---\n\n")
        logger.info(f"Markdown complet des articles sauvegard√© dans {output_path}")
        return output_path

def main():
    """
    Fonction principale pour g√©n√©rer le r√©sum√© hebdomadaire.
    """
    try:
        dotenv.load_dotenv()
        # Initialiser le digest hebdomadaire
        digest = WeeklyDigest()

        # G√©n√©rer le r√©sum√© avec tous les articles de la semaine (limit √©lev√©)
        summary = digest.generate_digest(limit=1000)

        # Sauvegarder le r√©sum√©
        output_path = digest.save_digest(summary)
        logger.info(f"G√©n√©ration du r√©sum√© hebdomadaire termin√©e. Fichier: {output_path}")
        print(f"R√©sum√© hebdomadaire g√©n√©r√© avec succ√®s: {output_path}")

        # G√©n√©rer le markdown complet des articles de la semaine
        digest.db_manager.connect()  # R√©-ouvrir la connexion pour la g√©n√©ration du markdown complet
        full_md_path = digest.save_full_articles_markdown(days=7, limit=1000)
        digest.db_manager.disconnect()  # Fermer la connexion apr√®s
        if full_md_path:
            print(f"Markdown complet des articles g√©n√©r√©: {full_md_path}")
        else:
            print("Aucun article √† inclure dans le markdown complet.")
    except Exception as e:
        logger.error(f"Erreur dans le programme principal: {str(e)}")
        print(f"Erreur: {str(e)}")

if __name__ == "__main__":
    main()
